{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-Recommender-IBM-Tutorial\n",
    "\n",
    "In this notebook, we will walk through the development of a Movie Recommendaiton engine. We will use the ratings dataset from `MovieLens` to train an `ALS model` using `Apache Spark ML` package. We expect this notebook to be run on `IBM Cloud` using `IBM Watson DataScience Offering`.\n",
    "\n",
    "\n",
    "**In this notebook we will walk through**\n",
    "- Loading the MovieLen dataset and performing some data exploration tasks\n",
    "- Building/Training a Matrix Factorization (ALS) model on the user-ratings data using Apache Spark\n",
    "- Serving recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisists \n",
    "\n",
    "In order to run this notebook successfully you will need to\n",
    "\n",
    " - create IBM DataScience Project \n",
    "   - create IBM ID from [here](https://dataplatform.cloud.ibm.com/home), if you don't have one already \n",
    "   - activate IBM Watson for your created account, if its not already activated\n",
    "   - create a new `DataScience` project \n",
    "   \n",
    " - download `MovieLens` dataset and upload it to your project as a `data asset`\n",
    "    - download the small-dataset from [here](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip) or the full-dataset from [here](http://files.grouplens.org/datasets/movielens/ml-latest.zip)\n",
    "    \n",
    "    - unzip the downloaded file\n",
    "    - upload `ratings.csv`, `movies.csv` and `links.csv` as data assets to your project\n",
    "   \n",
    "  - create a new notebook `from url`\n",
    "    - provide the url for this notebook\n",
    "    - use the default pyspark environment with `python-3.5` and `spark-2.3`\n",
    "      - this environment provides an `Anaconda` runtime preloaded with the most popular data science libraries\n",
    "    \n",
    " - create a valid TMDB APIKEY (to show movie posters while serving recommendations). More information on how to create a TMDB APIKEY can be found [here](https://developers.themoviedb.org/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify python version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify python version\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify spark is running  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify spark is running \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading The Data & Data Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily connect your data assets to your notebook by clicking on `find and add data` on the top right corner of your notebook. You only need to select the data asset you want to load, click on the corresponding `insert to code` and finally `insert SparkSession DataFrame` to create a dataframe from your data-asset. This is how we will load our ratings, movies and links datasets into spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ratings Data\n",
    "\n",
    "Each row of the `DataFrame` consists of a `userId`, `movieId`, `timestamp` and `rating` given by the user to the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': 'Hx-7hqkD_F6DolqjFF8oMU1FtFtHllaXGWuhIV-LRsUx',\n",
    "    'service_id': 'iam-ServiceId-4dc09431-b807-4651-a98c-cb835538e744',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_5d2c1a44e4274f0ca732c9cf847c65de_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "bucket_id='recommendationengineswalkthrough-donotdelete-pr-ufctuswrrmk82o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ratings data\n",
    "# drop timestamp column since its not used in building the model\n",
    "\n",
    "ratings_df = spark.read\\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load(cos.url('ratings.csv', bucket_id)) \\\n",
    "    .drop('timestamp') \\\n",
    "    .repartition(4) \\\n",
    "    .cache()\n",
    "\n",
    "ratings_count=ratings_df.count()\n",
    "print(\"ratings_count: {}\".format(ratings_count))\n",
    "\n",
    "ratings_df.show(5, truncate=False)\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Data Exploration [Users]\n",
    "In the following cells we show \n",
    " - the number of users\n",
    " - the minimum number of ratings given by a user\n",
    " - the maximum number of ratings given by a user \n",
    " - the average number of ratings given by a user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "n_users = ratings_df.select('userId').distinct().count()\n",
    "min_user_rating_count = ratings_df.groupby('userId').count().sort(col('count').asc()).take(1)[0]['count']\n",
    "max_user_rating_count = ratings_df.groupby('userId').count().sort(col('count').desc()).take(1)[0]['count']\n",
    "avg_user_rating_count = ratings_df.groupby('userId').count().groupby().avg('count').take(1)[0]['avg(count)']\n",
    "\n",
    "print(\"n_users: {}\".format(n_users))\n",
    "print('min_user_rating_count: {}'.format(min_user_rating_count))\n",
    "print('max_user_rating_count: {}'.format(max_user_rating_count))\n",
    "print('avg_user_rating_count: {}'.format(avg_user_rating_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a more efficient way (multiple aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "ratings_df.groupby('userId').count() \\\n",
    "    .agg(\n",
    "        F.count('count').alias('n_users'), \n",
    "        F.min('count').alias('min_user_rating_count'), \n",
    "        F.max('count').alias('max_user_rating_count'), \n",
    "        F.avg('count').alias('avg_user_rating_count')\n",
    "        ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using built in describe function in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"summary statistics user ratings counts\")\n",
    "ratings_df.groupBy(\"userId\").count().select('count').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Data Exploration [Movies]\n",
    "In the following cells we show \n",
    " - the number of movies\n",
    " - the minimum number of ratings given to a movie\n",
    " - the maximum number of ratings given to a movie\n",
    " - the average number of ratings given to a movie\n",
    "\n",
    "using built in `describe` function in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"summary statistics movies ratings counts\")\n",
    "ratings_df.groupBy(\"movieId\").count().select('count').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Data Exploration [Ratings]\n",
    "In the following cells we show/plot the distribution of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "ratings_df.groupby('rating').count().sort(asc('rating')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sorted list of ratings counts (to use in plots)\n",
    "x = ratings_df.groupBy(\"rating\").count().collect()\n",
    "\n",
    "# sort in place based on ratings (i.e. from 0.5 -- 5)\n",
    "x.sort(key=lambda x: x['rating'], reverse=False)\n",
    "sorted_ratings_counts_list = list(map(lambda x: x['count'], x))\n",
    "print(\"sorted_ratings_counts_list: {}\".format(sorted_ratings_counts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_groups = 10\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "x_labels=list(map(lambda x: str(x), list(np.arange(0.5,5.5,0.5))))\n",
    "bar_width = 0.5\n",
    "opacity = 0.4\n",
    "\n",
    "rects1 = plt.bar(index, sorted_ratings_counts_list, bar_width, alpha=opacity, color='b', label='ratings')\n",
    "plt.xlabel('ratings')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.title('Distribution of ratings')\n",
    "\n",
    "plt.xticks(index + bar_width, x_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Data Exploration [Other]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.stat.crosstab(\"userId\", \"rating\").show(5)\n",
    "ratings_df.stat.crosstab(\"movieId\", \"rating\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect User Top Ratings at Driver\n",
    "In the following cells we will collect users TOP ratings at SparkDriver. This step is only performed for the purpose of this tutorial, as the users top ratings will be displayed later with recommendations, and is not required in a typical recommenaditon engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "user_ratings_df = ratings_df.select('userId', struct('movieId', 'rating').alias('movieAndRating'))\n",
    "user_ratings_df.show(5, truncate=False)\n",
    "user_ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "user_ratings_df = user_ratings_df \\\n",
    "    .groupby('userId') \\\n",
    "    .agg(collect_list('movieAndRating').alias('userRatings'))\n",
    "\n",
    "user_ratings_df.show(5)\n",
    "user_ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "user_ratings_schema = user_ratings_df.select('userRatings').schema.fields[0].dataType\n",
    "print(\"user_ratings_schema: {}\".format(user_ratings_schema))\n",
    "\n",
    "def sort_and_limit(lst, n=5, desc=True):\n",
    "    return sorted(lst, key=lambda x: x[1], reverse=desc)[0:n]\n",
    "sort_and_limit_udf = udf(sort_and_limit, user_ratings_schema)\n",
    "\n",
    "l=[[1,2],[3,4],[3,2],[0,1], [13,2],[10,1],[1,10],[3,3]]\n",
    "sort_and_limit(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df = user_ratings_df.select('userId', sort_and_limit_udf('userRatings').alias('userRatings'))\n",
    "\n",
    "user_ratings_df.show(5, truncate=False)\n",
    "user_ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = user_ratings_df.rdd.map(lambda r: (r['userId'], r['userRatings'])).collectAsMap()\n",
    "len(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=240\n",
    "user_ratings[240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Movies Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `movies.csv` contains the `movieId`, `title` and `genres` for each movie. As you can see, the `genres` field is a bit tricky to use, as the genres are in the form of one string delimited by the `|` character: `Adventure|Animation|Children|Comedy|Fantasy`. Additionally the movie `title` string contains the `release year`, which is a separate field we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from CSV\n",
    "movies_df = spark.read\\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load(cos.url('movies.csv', bucket_id)) \\\n",
    "    .repartition(2)\n",
    "\n",
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Data Transformation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a user-defined function (UDF) to extract this delimited string into a list of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "# define a UDF to convert the raw genres string to an array of genres and lowercase\n",
    "def extract_genres(genres):\n",
    "    return genres.lower().split(\"|\")\n",
    "extract_genres_udf = udf(extract_genres , ArrayType(StringType()))\n",
    "extract_genres(\"Adventure|Animation|Children|Comedy|Fantasy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a UDF to extract the release year from the title using a Python regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "import re\n",
    "\n",
    "# define a UDF to extract the release year from the title, and return the new title and year in a struct type\n",
    "def extract_year(title):\n",
    "    result = re.search(\"\\(\\d{4}\\)\", title)\n",
    "    try:\n",
    "        if result:\n",
    "            group = result.group()\n",
    "            year = group[1:-1]\n",
    "            start_pos = result.start()\n",
    "            title = title[:start_pos-1]\n",
    "            return (title, year)\n",
    "        else:\n",
    "            return (title, 1970)\n",
    "    except:\n",
    "        return (title, 1970)\n",
    "\n",
    "extract_year_udf = udf(extract_year,\\\n",
    "                   StructType([StructField(\"title\", StringType(), True),\\\n",
    "                               StructField(\"release_date\", StringType(), True)]))\n",
    "    \n",
    "\n",
    "extract_year(\"MovieExample (2010)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df \\\n",
    "    .select(\"movieId\", \n",
    "            extract_year_udf(\"title\").title.alias(\"title\"), \n",
    "            extract_year_udf(\"title\").release_date.alias(\"release_date\"),\n",
    "            extract_genres_udf(\"genres\").alias(\"genres\"))\n",
    "\n",
    "n_movies = movies_df.count()\n",
    "print(\"n_movies: {}\".format(n_movies))\n",
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Data Transformation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we join the `links.csv` data to `movies` so that there is an id for _The Movie Database_ corresponding to each movie. We will use this id to retrieve movie poster images when displaying recommendations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data = spark.read\\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load(cos.url('links.csv', bucket_id)) \\\n",
    "    .repartition(2)\n",
    "\n",
    "link_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join movies with links to get TMDB id\n",
    "movies_df = movies_df.join(link_data, on='movieId')\\\n",
    "    .select(movies_df.columns + ['tmdbId'])\n",
    "\n",
    "n_movies = movies_df.count()\n",
    "print(\"n_movies: {}\".format(n_movies))\n",
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Transformed Movies Data at Driver\n",
    "We will maintain a mapping from Key (movieId) to a Dict (movieInfo). This step is only performed for the purpose of this tutorial; typically movies information is stored in some Database, and is external to the recommendaiton application (we only need the ratings data from collaborative filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_map_df = movies_df.select('movieId', struct(movies_df.columns).alias('movieInfo'))\n",
    "movies_map_df.show(5, truncate=False)\n",
    "movies_map_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_info = movies_map_df.rdd.map(lambda r: (r['movieId'], r['movieInfo'])).collectAsMap()\n",
    "len(movies_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quering a movie from redis\n",
    "movie_id=10\n",
    "print(movies_info[movie_id])\n",
    "print()\n",
    "print(movies_info[movie_id]['title'])\n",
    "print(movies_info[movie_id]['tmdbId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "From the previous steps we now have\n",
    " - ratings_df: spark dataframe that will be used in training the ALS Matrix Factorization Model \n",
    " - user_ratings: mapping from userId to list of top 5 ratings done by the user (sorted by rating in desc order)\n",
    " - movies_info: mapping from movieId to movieInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratings DataFrame\")\n",
    "ratings_df.show(5, truncate=False)\n",
    "print()\n",
    "print(\"Query Movie details about movieId: {}\".format(movie_id))\n",
    "print(movies_info[movie_id])\n",
    "print()\n",
    "print(\"Query top User ratings for userId: {}\".format(uid))\n",
    "print(user_ratings[uid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a recommmender model on the ratings data\n",
    "\n",
    "In the following blocks we will use the ratings data to build a collaborative filtering recommendation model using Apache Spark's implementation of ALS. \n",
    "\n",
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) is a recommendation approach that is effectively based on the \"wisdom of the crowd\". It makes the assumption that, if two people share similar preferences, then the things that one of them prefers could be good recommendations to make to the other. In other words, if user A tends to like certain movies, and user B shares some of these preferences with user A, then the movies that user A likes, that user B _has not yet seen_, may well be movies that user B will also like.\n",
    "\n",
    "In a similar manner, we can think about _items_ as being similar if they tend to be rated highly by the same people, on average. \n",
    "\n",
    "Hence these models are based on the combined, collaborative preferences and behavior of all users in aggregate. They tend to be very effective in practice (provided you have enough preference data to train the model). The ratings data we have is a form of _explicit preference data_, perfect for training collaborative filtering models.\n",
    "\n",
    "\n",
    "### Alternating Least Squares\n",
    "Alternating Least Squares (ALS) is a specific algorithm for solving a type of collaborative filtering model known as [matrix factorization (MF)](https://en.wikipedia.org/wiki/Matrix_decomposition). The core idea of MF is to represent the ratings as a _user-item ratings matrix_. The entries in this matrix are the ratings given by users to movies. Typically, that _user-item ratings matrix_ has _missing entries_ because not all users have rated all movies. In this situation we refer to the data as _sparse_.\n",
    "\n",
    "\n",
    "![als-diagram.png](https://github.com/ThinkBigEg/movie-recommendation-tutorial/blob/master/als-diagram.png?raw=true)\n",
    "\n",
    "MF methods aim to find two much smaller matrices (one representing the _users_ and the other the _items_) that, when multiplied together, re-construct the original ratings matrix as closely as possible. This is know as _factorizing_ the original matrix, hence the name of the technique.\n",
    "\n",
    "The two smaller matrices are called _factor matrices_ (or _latent features_). The user and movie factor matrices are illustrated on the right in the diagram above. The idea is that each user factor vector is a compressed representation of the user's preferences and behavior. Likewise, each item factor vector is a compressed representation of the item. Once the model is trained, the factor vectors can be used to make recommendations, which is what you will do in the following sections.\n",
    "\n",
    "__Further reading:__\n",
    "* [Spark MLlib Collaborative Filtering](http://spark.apache.org/docs/latest/ml-collaborative-filtering.html)\n",
    "* [Alternating Least Squares and collaborative filtering](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/)\n",
    "* [Quora question on Alternating Least Squares](https://www.quora.com/What-is-the-Alternating-Least-Squares-method-in-recommendation-systems-And-why-does-this-algorithm-work-intuition-behind-this)\n",
    "\n",
    "Fortunately, Spark's MLlib machine learning library has a scalable, efficient implementation of matrix factorization built in, which we can use to train our recommendation model.\n",
    "\n",
    "**Spark's ALS takes the following inputs:**\n",
    " - numBlocks: the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10)\n",
    " - rank: the number of latent factors in the model (defaults to 10)\n",
    " - maxIter: the maximum number of iterations to run (defaults to 10)\n",
    " - regParam: the regularization parameter in ALS - used to prevent overfitting (defaults to 0.1)\n",
    " - implicitPrefs: specifies whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (defaults to false, which means it's using explicit feedback)\n",
    "   - alpha: This is a parameter applicable to the implicit feedback variant of ALS, which governs the baseline confidence in preference observations (defaults to 1.0)\n",
    " - nonnegative: This parameter specifies whether or not to use non-negative constraints for least squares (defaults to false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df,test_df = ratings_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "rank = 20  # number of latent factors\n",
    "maxIter = 10\n",
    "regParam=0.01  # prevent overfitting\n",
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", \n",
    "          regParam=regParam, \n",
    "          rank=rank, \n",
    "          maxIter=maxIter,\n",
    "          coldStartStrategy=\"drop\",\n",
    "          seed=12)\n",
    "\n",
    "model = als.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show(5, truncate=False)\n",
    "predictions_df = model.transform(test_df)\n",
    "predictions_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "error = evaluator.evaluate(predictions_df)\n",
    "\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Tuning: model selection and hyperparameter tuning\n",
    "\n",
    "An important task in ML is `model selection`, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LogisticRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately.\n",
    "\n",
    "MLlib supports model selection using tools such as `CrossValidator` and `TrainValidationSplit`. From which we are using the `CrossValidator`. These tools require the following items:\n",
    " - Estimator: algorithm or Pipeline to tune\n",
    " - Set of ParamMaps: parameters to choose from, sometimes called a “parameter grid” to search over\n",
    " - Evaluator: metric to measure how well a fitted Model does on held-out test data\n",
    "\n",
    "At a high level, these model selection tools work as follows:\n",
    " - They split the input data into separate training and test datasets.\n",
    " - For each (training, test) pair, they iterate through the set of ParamMaps:\n",
    "   - For each ParamMap, they fit the Estimator using those parameters, get the fitted Model, and evaluate the Model’s performance using the Evaluator.\n",
    " - They select the Model produced by the best-performing set of parameters.\n",
    " \n",
    "The Evaluator can be a RegressionEvaluator for regression problems, a BinaryClassificationEvaluator for binary data, or a MulticlassClassificationEvaluator for multiclass problems. The default metric used to choose the best ParamMap can be overridden by the setMetricName method in each of these evaluators.\n",
    "\n",
    "To help construct the parameter grid, we are using the ParamGridBuilder utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "\n",
    "# creating parameter grid (parameters to be tuned)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 15, 20],) \\\n",
    "    .addGrid(als.maxIter, [10, 15],) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.2, 0.3],) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "crossval = CrossValidator(estimator=als, \n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cv_model = crossval.fit(training_df)\n",
    "best_model=cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rank=best_model.rank\n",
    "best_regParm=best_model._java_obj.parent().getRegParam()\n",
    "best_iterations=best_model._java_obj.parent().getMaxIter()\n",
    "\n",
    "print(\"best_rank: {}\".format(best_rank))\n",
    "print(\"best_regParm: {}\".format(best_regParm))\n",
    "print(\"best_iterations: {}\".format(best_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "predictions_df = best_model.transform(test_df)\n",
    "predictions_df.show(5, truncate=False)\n",
    "\n",
    "\n",
    "error = evaluator.evaluate(predictions_df)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "- userFactors\n",
    "- itemFactors\n",
    "- recommendForAllUsers\n",
    "- recommendForAllItems\n",
    "- recommendForUserSubset\n",
    "- recommendForItemSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.userFactors.show(5)\n",
    "best_model.itemFactors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Persisting the model\n",
    "\n",
    "For the purpose of this tutorial we are just collecting the users top 5 recommendaitons at the driver.\n",
    "First we will generate the users top 5 recommendations from the ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_recs=5\n",
    "users_recommendations_df = best_model.recommendForAllUsers(n_recs)\n",
    "\n",
    "users_recommendations_df.show(5, truncate=False)\n",
    "users_recommendations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_recommendations = users_recommendations_df.rdd.map(lambda r: (r['userId'], r['recommendations'])).collectAsMap()\n",
    "len(users_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=240\n",
    "users_recommendations[uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Serving Recommendations\n",
    "\n",
    "* Given a user, find the movies with the highest predicted rating\n",
    "* Display the results as an HTML table in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ratings(user_map, uid):\n",
    "    u_ratings = user_map[uid]\n",
    "    res = []\n",
    "    for r in u_ratings:\n",
    "        movieId = r['movieId']\n",
    "        rating = r['rating']\n",
    "        movie_info = movies_info[movieId].asDict()\n",
    "        movie_info['rating'] = rating\n",
    "        res.append(movie_info)\n",
    "    return res\n",
    "\n",
    "    \n",
    "\n",
    "print(get_user_ratings(user_ratings, 240))\n",
    "print()\n",
    "print(get_user_ratings(users_recommendations, 240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetching movie posters from TMdb API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tmdbsimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "import tmdbsimple as tmdb\n",
    "from tmdbsimple import APIKeyError\n",
    "# TMdb API key\n",
    "tmdb.API_KEY = '4b577aa255b7f9215deb1ed3933b0d7c'\n",
    "\n",
    "def get_poster_url(tmdbid):\n",
    "    \"\"\"Fetch movie poster image URL from TMDb API given a tmdbId\"\"\"\n",
    "    IMAGE_URL = 'https://image.tmdb.org/t/p/w500'  # base URL for TMDB poster images\n",
    "    try:\n",
    "        try:\n",
    "            movie = tmdb.Movies(tmdbid).info()\n",
    "            poster_url = IMAGE_URL + movie['poster_path'] if 'poster_path' in movie and movie['poster_path'] is not None else \"\"\n",
    "            return poster_url\n",
    "        except APIKeyError as ae:\n",
    "            return \"KEY_ERR\"\n",
    "    except Exception as me:\n",
    "        return \"NA\"\n",
    "\n",
    "movie_id=2\n",
    "movie_info=movies_info[movie_id]\n",
    "\n",
    "try:\n",
    "    print(\"movie title: {}\".format(movie_info['title']))\n",
    "    movie_poster_url =  get_poster_url(movie_info['tmdbId'])\n",
    "    print(\"movie_poster_url: {}\".format(movie_poster_url))\n",
    "    display(Image(movie_poster_url, width=200))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Cannot import tmdbsimple, no movie posters will be displayed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_user_recs(uid):\n",
    "    # utility function that constructs an HTML of ratings\n",
    "    # ---------------------------------------------------\n",
    "    def construct_recommendations_html(ratings):\n",
    "        html = \"<table border=0>\"\n",
    "        i = 0\n",
    "        for movie in ratings:\n",
    "            movie_im_url = get_poster_url(movie['tmdbId'])\n",
    "            movie_title = movie['title']\n",
    "            movie_rating = movie['rating']\n",
    "            #\n",
    "            html += \"<td><h5>{}</h5><img src={} width=150></img></td><td><h5>{:.2f}</h5></td>\".format(movie_title, movie_im_url, movie_rating)\n",
    "\n",
    "            i += 1\n",
    "            if i % 5 == 0:\n",
    "                html += \"</tr><tr>\"\n",
    "        html += \"</tr></table>\"\n",
    "        return html\n",
    "    \n",
    "    \n",
    "    # display the movies that this user has rated highly \n",
    "    # -------------------------------------------------\n",
    "    top_rated = get_user_ratings(user_ratings, uid)  # List [Dict (movieId, tmdbId, rating, title)]\n",
    "    display(HTML(\"<h2>Top Ratings By User: {}</h2>\".format(uid)))\n",
    "    display(HTML(\"<h4>The user has rated the following movies highly:</h4>\"))\n",
    "    top_rated_html = construct_recommendations_html(top_rated)\n",
    "    display(HTML(top_rated_html))\n",
    "    \n",
    "    \n",
    "    # display the movies that this user has the highest score\n",
    "    # -------------------------------------------------    \n",
    "    recommendations = get_user_ratings(users_recommendations, uid)\n",
    "    display(HTML(\"<br/>\"))\n",
    "    display(HTML(\"<h2>Top Recommendations For User: {}</h2>\".format(uid)))\n",
    "    display(HTML(\"<h4>The user is recommended these movies:</h4>\"))\n",
    "    recommendations_html = construct_recommendations_html(recommendations)\n",
    "    display(HTML(recommendations_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Serving Recommendations\n",
    "\n",
    "Now, we're ready to generate movie recommendations, personalized for specific users.\n",
    "Given a user, we can recommend movies to that user based on the predicted ratings from our model. Recall that the collaborative filtering model means that, at a high level, we will recommend movies _liked by other users who liked the same movies as the given user_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_user_recs(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we are using a very small dataset, the results may not be too good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
